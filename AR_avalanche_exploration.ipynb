{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmospheric Rivers: explore spatiotemporal relationship with avalanches \n",
    "\n",
    "This notebook will import the revised avalanche database table, a reference point layer, and the AR events shapefile. A spatiotemporal relationship is explored.\n",
    "  \n",
    "  \n",
    "\n",
    "_Note: the avalanche database table was revised to remove duplicate events, and to choose the nearest populated place to represent the avalanche location. This allows simple linking to a reference point layer, avoiding manual population of coordinates for avalanche events. This method should not produce appreciable errors when considering the scale of this analysis. If the results table will be further filtered by proximity, consider using a value that respects the spatial ambiguity of both the modeled AR boundaries and the generalized avalanche locations._\n",
    "  \n",
    "  \n",
    "\n",
    "For each avalanche event, AR landfall events occuring within the 7 days prior to the avalanche are retained, with distance to the closest AR recorded. Distances of zero indicate the avalanche occurred within the AR polygon boundary. Attributes of AR duration and strength are joined to the avalanche database table and exported as a CSV. Any NA values in the resulting table indicate that no AR event occurred within 7 days prior to the avalanche.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import timedelta\n",
    "\n",
    "from config import landfall_events_fp, ak_pts, avy_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in AK points reference, avalanche db, and AR events shp\n",
    "ak_pts = pd.read_csv(ak_pts)\n",
    "avy = pd.read_csv(avy_db, skiprows=1)\n",
    "ars = gpd.read_file(landfall_events_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute duration of each event and event intensity (strength/duration)\n",
    "ars['start'] = pd.to_datetime(ars['start'])\n",
    "ars['end'] = pd.to_datetime(ars['end'])\n",
    "\n",
    "for i in ars.index:\n",
    "    #after subtracting, add 6hrs as minimum event length.... this assures a single timestep event is not zero duration!\n",
    "    ars.loc[i, 'Duration_hrs'] = ((ars['end'][i]-ars['start'][i]).total_seconds()/3600) + 6\n",
    "\n",
    "ars['AR_Total_Intensity'] = ars['sumtot_str']/ars['Duration_hrs']\n",
    "ars['AR_Relative_Intensity'] = ars['sumrel_str']/ars['Duration_hrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1570/107876706.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['Avalanche Date'])\n"
     ]
    }
   ],
   "source": [
    "#join avalanche db to AK points, clean table for processing, and add geometry using lat/lon info\n",
    "df = avy.merge(ak_pts, how='left', left_on='Loc', right_on='name')\n",
    "df['date'] = pd.to_datetime(df['Avalanche Date'])\n",
    "df.rename(columns={'Avalanche ID (new)' : 'ID'}, inplace=True)\n",
    "df = df[['ID', 'date', 'Trigger', 'name', 'latitude', 'longitude']]\n",
    "df['geometry'] = gpd.GeoSeries.from_xy(df['longitude'], df['latitude'], crs=ars.crs)\n",
    "df = gpd.GeoDataFrame(df, geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert both to 3338 for minimum distance measurement\n",
    "df_3338 = df.to_crs('EPSG:3338')\n",
    "ars_3338 = ars.to_crs('EPSG:3338')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a duration dictionary with AR ids and their start and end dates\n",
    "ids = ars_3338['event_id']\n",
    "starts = ars_3338['start']\n",
    "ends = ars_3338['end']\n",
    "\n",
    "ars_dict = {i : [s, e] for i, s, e in zip(ids, starts, ends)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~2 min\n",
    "#for each avalanche ID, create a date range of 1 week preceding the avalanche\n",
    "#then for each AR in the duration dictionary, compare the AR date range to the avalanche date range\n",
    "#if overlapping, add the avalanche ID and list of one or more ARs to a new dictionary\n",
    "\n",
    "avy_ars = {}\n",
    "\n",
    "for avy in df_3338['ID'].unique():\n",
    "\n",
    "    #create a range of dates from 1 week before avalanche to avalanche\n",
    "    avy_end_date = df_3338[df_3338['ID']==avy]['date']\n",
    "    avy_start_date = avy_end_date - timedelta(days=7)\n",
    "\n",
    "    ars_sub = []\n",
    "\n",
    "    #check if the avalanche date range overlaps any of the AR ranges\n",
    "    for k in ars_dict.keys():\n",
    "        ar_start_date = ars_dict[k][0]\n",
    "        ar_end_date = ars_dict[k][1]\n",
    "\n",
    "        #add to list if overlapping\n",
    "        if (avy_start_date <= ar_end_date).bool() & (ar_start_date <= avy_end_date).bool():\n",
    "            ars_sub.append(k)\n",
    "\n",
    "    #if any ARs overlap, add list to dictionary    \n",
    "    if len(ars_sub) > 0:\n",
    "        avy_ars[avy] = [ars_sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the dictionary of ARs occuring within 1 week prior to avalanche, find the AR with minimum distance to the AR\n",
    "#record the AR id and its distance from avalanche (in km) in a new dict\n",
    "#distance values of 0 indicate the point is inside the AR polygon\n",
    "#if more than one AR have the same distance of zero, only the first is retained\n",
    " \n",
    "avy_ars_min_dist = {}\n",
    "\n",
    "for a in avy_ars:\n",
    "    pt_a = df_3338.loc[df_3338['ID']==a].geometry\n",
    "\n",
    "    dists = []\n",
    "\n",
    "    for b in avy_ars[a][0]:\n",
    "        pt_b = ars_3338.iloc[[b]].geometry\n",
    "        dist = pt_a.distance(pt_b, align=False)\n",
    "        dists.append(round(dist.iloc[0]/1000)) #convert m to km, and round to integer\n",
    "    \n",
    "    avy_ars_min_dist[a] = [avy_ars[a][0][pd.Series(dists).idxmin()], dists[pd.Series(dists).idxmin()]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert minimum distance dict to dataframe and join to original avalanche points\n",
    "#then join AR properties\n",
    "result = df_3338.merge(\n",
    "    pd.DataFrame.from_dict(avy_ars_min_dist, orient='index', columns=['AR_ID', 'min_km']),\n",
    "     how='left', left_on='ID', right_index=True).merge(\n",
    "        ars_3338, how='left', left_on='AR_ID', right_on='event_id')\n",
    "\n",
    "#clean results table and compute intensity using hours (strength over time)\n",
    "result = result.rename(columns={'ID':'Avalanche_ID',\n",
    "'date':'Date', \n",
    "'name':'Location',\n",
    "'latitude':'Latitude',\n",
    "'longitude':'Longitude', \n",
    "'AR_ID':'AR_Event_ID', \n",
    "'min_km':'Avalanche_to_AR_Distance_km',\n",
    "'start':'Start',\n",
    "'end':'End',\n",
    "'sumtot_str':'Cumulative_AR_Total_Strength',\n",
    "'sumrel_str':'Cumulative_AR_Relative_Strength'\n",
    "}).drop(columns=['event_id', 'geometry_x', 'geometry_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avalanche_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Trigger</th>\n",
       "      <th>Location</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>AR_Event_ID</th>\n",
       "      <th>Avalanche_to_AR_Distance_km</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Cumulative_AR_Total_Strength</th>\n",
       "      <th>Cumulative_AR_Relative_Strength</th>\n",
       "      <th>Duration_hrs</th>\n",
       "      <th>AR_Total_Intensity</th>\n",
       "      <th>AR_Relative_Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1988-04-09</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>61.1817</td>\n",
       "      <td>-149.9930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>1992-11-08</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>61.1817</td>\n",
       "      <td>-149.9930</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1992-11-08 00:00:00</td>\n",
       "      <td>1992-11-09 06:00:00</td>\n",
       "      <td>8038923.0</td>\n",
       "      <td>2162.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>223303.416667</td>\n",
       "      <td>60.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>1993-01-10</td>\n",
       "      <td>Human</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>61.1817</td>\n",
       "      <td>-149.9930</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1992-12-26 12:00:00</td>\n",
       "      <td>1993-01-06 18:00:00</td>\n",
       "      <td>37830618.0</td>\n",
       "      <td>8651.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>137067.456522</td>\n",
       "      <td>31.344203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>1999-03-26</td>\n",
       "      <td>no data</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>61.1817</td>\n",
       "      <td>-149.9930</td>\n",
       "      <td>396.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999-03-23 00:00:00</td>\n",
       "      <td>1999-03-25 00:00:00</td>\n",
       "      <td>4392385.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>81340.462963</td>\n",
       "      <td>14.351852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>2000-11-11</td>\n",
       "      <td>Human</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>61.1817</td>\n",
       "      <td>-149.9930</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000-11-03 00:00:00</td>\n",
       "      <td>2000-11-11 12:00:00</td>\n",
       "      <td>42936833.0</td>\n",
       "      <td>7400.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>204461.109524</td>\n",
       "      <td>35.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>128</td>\n",
       "      <td>2010-02-13</td>\n",
       "      <td>no data</td>\n",
       "      <td>Whittier</td>\n",
       "      <td>60.7730</td>\n",
       "      <td>-148.6840</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010-02-05 06:00:00</td>\n",
       "      <td>2010-02-06 12:00:00</td>\n",
       "      <td>3368417.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>93567.138889</td>\n",
       "      <td>32.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>130</td>\n",
       "      <td>2010-02-18</td>\n",
       "      <td>no data</td>\n",
       "      <td>Moose Pass</td>\n",
       "      <td>60.4876</td>\n",
       "      <td>-149.3670</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010-02-10 00:00:00</td>\n",
       "      <td>2010-02-11 18:00:00</td>\n",
       "      <td>1544453.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32176.104167</td>\n",
       "      <td>11.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>132</td>\n",
       "      <td>2010-05-29</td>\n",
       "      <td>no data</td>\n",
       "      <td>Petersville</td>\n",
       "      <td>62.3730</td>\n",
       "      <td>-150.7332</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2010-05-29 00:00:00</td>\n",
       "      <td>2010-05-29 00:00:00</td>\n",
       "      <td>390579.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>65096.500000</td>\n",
       "      <td>39.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>158</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>Human</td>\n",
       "      <td>Chitina</td>\n",
       "      <td>61.5158</td>\n",
       "      <td>-144.4370</td>\n",
       "      <td>1291.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>2015-03-26 00:00:00</td>\n",
       "      <td>2015-03-30 18:00:00</td>\n",
       "      <td>13891310.0</td>\n",
       "      <td>3682.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>115760.916667</td>\n",
       "      <td>30.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>174</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>no data</td>\n",
       "      <td>Wiseman</td>\n",
       "      <td>67.4100</td>\n",
       "      <td>-150.1070</td>\n",
       "      <td>1394.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2017-02-24 00:00:00</td>\n",
       "      <td>2017-02-26 12:00:00</td>\n",
       "      <td>8156155.0</td>\n",
       "      <td>1782.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>123578.106061</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avalanche_ID       Date  Trigger     Location  Latitude  Longitude  \\\n",
       "0               5 1988-04-09  Natural    Anchorage   61.1817  -149.9930   \n",
       "1              18 1992-11-08  Natural    Anchorage   61.1817  -149.9930   \n",
       "2              20 1993-01-10    Human    Anchorage   61.1817  -149.9930   \n",
       "3              42 1999-03-26  no data    Anchorage   61.1817  -149.9930   \n",
       "4              66 2000-11-11    Human    Anchorage   61.1817  -149.9930   \n",
       "..            ...        ...      ...          ...       ...        ...   \n",
       "164           128 2010-02-13  no data     Whittier   60.7730  -148.6840   \n",
       "165           130 2010-02-18  no data   Moose Pass   60.4876  -149.3670   \n",
       "166           132 2010-05-29  no data  Petersville   62.3730  -150.7332   \n",
       "167           158 2015-04-01    Human      Chitina   61.5158  -144.4370   \n",
       "168           174 2017-02-27  no data      Wiseman   67.4100  -150.1070   \n",
       "\n",
       "     AR_Event_ID  Avalanche_to_AR_Distance_km               Start  \\\n",
       "0            NaN                          NaN                 NaT   \n",
       "1           44.0                          0.0 1992-11-08 00:00:00   \n",
       "2           53.0                          0.0 1992-12-26 12:00:00   \n",
       "3          396.0                          0.0 1999-03-23 00:00:00   \n",
       "4          486.0                          0.0 2000-11-03 00:00:00   \n",
       "..           ...                          ...                 ...   \n",
       "164       1006.0                          0.0 2010-02-05 06:00:00   \n",
       "165       1007.0                          0.0 2010-02-10 00:00:00   \n",
       "166       1026.0                         64.0 2010-05-29 00:00:00   \n",
       "167       1291.0                        204.0 2015-03-26 00:00:00   \n",
       "168       1394.0                         48.0 2017-02-24 00:00:00   \n",
       "\n",
       "                    End  Cumulative_AR_Total_Strength  \\\n",
       "0                   NaT                           NaN   \n",
       "1   1992-11-09 06:00:00                     8038923.0   \n",
       "2   1993-01-06 18:00:00                    37830618.0   \n",
       "3   1999-03-25 00:00:00                     4392385.0   \n",
       "4   2000-11-11 12:00:00                    42936833.0   \n",
       "..                  ...                           ...   \n",
       "164 2010-02-06 12:00:00                     3368417.0   \n",
       "165 2010-02-11 18:00:00                     1544453.0   \n",
       "166 2010-05-29 00:00:00                      390579.0   \n",
       "167 2015-03-30 18:00:00                    13891310.0   \n",
       "168 2017-02-26 12:00:00                     8156155.0   \n",
       "\n",
       "     Cumulative_AR_Relative_Strength  Duration_hrs  AR_Total_Intensity  \\\n",
       "0                                NaN           NaN                 NaN   \n",
       "1                             2162.0          36.0       223303.416667   \n",
       "2                             8651.0         276.0       137067.456522   \n",
       "3                              775.0          54.0        81340.462963   \n",
       "4                             7400.0         210.0       204461.109524   \n",
       "..                               ...           ...                 ...   \n",
       "164                           1178.0          36.0        93567.138889   \n",
       "165                            532.0          48.0        32176.104167   \n",
       "166                            236.0           6.0        65096.500000   \n",
       "167                           3682.0         120.0       115760.916667   \n",
       "168                           1782.0          66.0       123578.106061   \n",
       "\n",
       "     AR_Relative_Intensity  \n",
       "0                      NaN  \n",
       "1                60.055556  \n",
       "2                31.344203  \n",
       "3                14.351852  \n",
       "4                35.238095  \n",
       "..                     ...  \n",
       "164              32.722222  \n",
       "165              11.083333  \n",
       "166              39.333333  \n",
       "167              30.683333  \n",
       "168              27.000000  \n",
       "\n",
       "[169 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 169 unique avalanche IDs in the revised database...\n",
      "149 of those avalanches have at least one landfalling AR occuring within the 7 days preceding the avalanche...\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(len(df_3338['ID'].unique())) + \" unique avalanche IDs in the revised database...\")\n",
    "print(str(len(avy_ars)) + \" of those avalanches have at least one landfalling AR occuring within the 7 days preceding the avalanche...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp_ar_avalanche",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
